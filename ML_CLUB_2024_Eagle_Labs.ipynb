{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNedYzcFasRZsPNUbCxsDKk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msamwelmollel/ML-CLUB-2024-Eagle-Labs/blob/main/ML_CLUB_2024_Eagle_Labs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "vA9y3iBaqWlf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Image\n",
        "\n",
        "# # Display the image with a specific width and height\n",
        "# Image('RAG.jpeg', width=600, height=400)"
      ],
      "metadata": {
        "id": "qbAPBDVmrRmI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kKj86Nbuqnb-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33R8T1BhU2Fw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c82a2c1-9210-4ea4-db9a-a7f959ece18d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -qU langchain langchain-openai langchain-mongodb langchain-experimental ragas pymongo tqdm chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import getpass\n",
        "# MONGODB_URI = getpass.getpass(\"Enter your MongoDB connection string:\")"
      ],
      "metadata": {
        "id": "1tXeF0X2428M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "62fZ9JftarTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY # getpass.getpass(\"Enter your OpenAI API Key:\")\n",
        "openai_client = OpenAI()"
      ],
      "metadata": {
        "id": "ooYU5OxF6neU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "web_loader = WebBaseLoader(\n",
        "    [\n",
        "        \"https://peps.python.org/pep-0483/\",\n",
        "        \"https://peps.python.org/pep-0008/\",\n",
        "        \"https://peps.python.org/pep-0257/\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "pages = web_loader.load()"
      ],
      "metadata": {
        "id": "gnk0ks2I63wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document = pages"
      ],
      "metadata": {
        "id": "0hoDBJPQbSkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x= Document(\n",
        "#     page_content=\"\\nThe goal of this PEP is to propose such a systematic way of defining types\\nfor type annotations of variables and functions using PEP 3107 syntax.\",\n",
        "#     metadata= {'source': 'https://peps.python.org/pep-0483/', 'title': 'PEP 483 – The Theory of Type Hints | peps.python.org', 'description': 'Python Enhancement Proposals (PEPs)', 'language': 'en'}\n",
        "# )"
      ],
      "metadata": {
        "id": "FqWURu5z7MLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "from langchain.schema import Document  # Assuming Document is from langchain.schema"
      ],
      "metadata": {
        "id": "WT9olt8ulFx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "def fixed_token_split(\n",
        "    docs: List[Document], chunk_size: int, chunk_overlap: int\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Fixed token chunking\n",
        "\n",
        "    Args:\n",
        "        docs (List[Document]): List of documents to chunk\n",
        "        chunk_size (int): Chunk size (number of tokens)\n",
        "        chunk_overlap (int): Token overlap between chunks\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: List of chunked documents\n",
        "    \"\"\"\n",
        "    splitter = TokenTextSplitter(\n",
        "        encoding_name=\"cl100k_base\", chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "zmYHzc017TdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import (\n",
        "    Language,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "\n",
        "def recursive_split(\n",
        "    docs: List[Document],\n",
        "    chunk_size: int,\n",
        "    chunk_overlap: int,\n",
        "    language: Optional[Language] = None,\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Recursive chunking\n",
        "\n",
        "    Args:\n",
        "        docs (List[Document]): List of documents to chunk\n",
        "        chunk_size (int): Chunk size (number of tokens)\n",
        "        chunk_overlap (int): Token overlap between chunks\n",
        "        language (Optional[Language], optional): Programming language enum. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: List of chunked documents\n",
        "    \"\"\"\n",
        "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "\n",
        "    if language is not None:\n",
        "        try:\n",
        "            separators = RecursiveCharacterTextSplitter.get_separators_for_language(\n",
        "                language\n",
        "            )\n",
        "        except (NameError, ValueError) as e:\n",
        "            print(f\"No separators found for language {language}. Using defaults.\")\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        encoding_name=\"cl100k_base\",\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=separators,\n",
        "    )\n",
        "    return splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "mYIRN_Pu8aRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "def semantic_split(docs: List[Document]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Semantic chunking\n",
        "\n",
        "    Args:\n",
        "        docs (List[Document]): List of documents to chunk\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: List of chunked documents\n",
        "    \"\"\"\n",
        "    splitter = SemanticChunker(\n",
        "        OpenAIEmbeddings(), breakpoint_threshold_type=\"percentile\"\n",
        "    )\n",
        "    return splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "sUr11NTyTIYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import RunConfig\n",
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "RUN_CONFIG = RunConfig(max_workers=4, max_wait=180)\n",
        "\n",
        "# generator with openai models\n",
        "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
        "critic_llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "generator = TestsetGenerator.from_langchain(generator_llm, critic_llm, embeddings)\n",
        "\n",
        "# Change resulting question type distribution\n",
        "distributions = {simple: 0.4, multi_context: 0.4, reasoning: 0.2}\n",
        "\n",
        "testset = generator.generate_with_langchain_docs(\n",
        "    pages, 10, distributions, run_config=RUN_CONFIG\n",
        ")"
      ],
      "metadata": {
        "id": "hSrws65TTrvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# questions = [item.question for item in testset.test_data]\n",
        "# ground_truths = [item.ground_truth for item in testset.test_data]\n",
        "# questions"
      ],
      "metadata": {
        "id": "GgAnWFAzn0v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from typing import List, Dict\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import context_precision, context_recall\n",
        "import nest_asyncio\n",
        "\n",
        "# Allow nested use of asyncio (used by Ragas)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Disable tqdm locks\n",
        "tqdm.get_lock().locks = []\n",
        "\n",
        "# Assuming testset is already defined\n",
        "QUESTIONS = [item.question for item in testset.test_data]\n",
        "GROUND_TRUTH = [item.ground_truth for item in testset.test_data]\n",
        "\n",
        "def create_vector_store(docs: List[Document]) -> Chroma:\n",
        "    embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "    return Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        embedding=embedding_function,\n",
        "        collection_name=\"chunking_evaluation\"\n",
        "    )\n",
        "\n",
        "def perform_eval(docs: List[Document]) -> Dict[str, float]:\n",
        "    eval_data = {\n",
        "        \"question\": QUESTIONS,\n",
        "        \"ground_truth\": GROUND_TRUTH,\n",
        "        \"contexts\": [],\n",
        "    }\n",
        "\n",
        "    print(\"Creating new Chroma vector store\")\n",
        "    vector_store = create_vector_store(docs)\n",
        "\n",
        "    print(\"Getting contexts for evaluation set\")\n",
        "    for question in tqdm(QUESTIONS):\n",
        "        eval_data[\"contexts\"].append(\n",
        "            [doc.page_content for doc in vector_store.similarity_search(question, k=3)]\n",
        "        )\n",
        "\n",
        "    dataset = Dataset.from_dict(eval_data)\n",
        "\n",
        "    print(\"Running evals\")\n",
        "    result = evaluate(\n",
        "        dataset=dataset,\n",
        "        metrics=[context_precision, context_recall],\n",
        "        run_config=RUN_CONFIG,\n",
        "        raise_exceptions=False,\n",
        "    )\n",
        "    return result\n",
        "\n",
        "\n",
        "# Initialize results list\n",
        "results = []\n",
        "\n",
        "# Evaluation loop\n",
        "for chunking_strategy in [\"Fixed token without overlap\", \"Fixed token with overlap\", \"Recursive with overlap\", \"Recursive Python splitter with overlap\"]:\n",
        "    for chunk_size in [100, 200, 500, 1000]:\n",
        "        chunk_overlap = int(0.15 * chunk_size)\n",
        "        print(f\"Evaluating: {chunking_strategy} - Chunk Size: {chunk_size}\")\n",
        "\n",
        "        if chunking_strategy == \"Fixed token without overlap\":\n",
        "            result = perform_eval(fixed_token_split(pages, chunk_size, 0))\n",
        "        elif chunking_strategy == \"Fixed token with overlap\":\n",
        "            result = perform_eval(fixed_token_split(pages, chunk_size, chunk_overlap))\n",
        "        elif chunking_strategy == \"Recursive with overlap\":\n",
        "            result = perform_eval(recursive_split(pages, chunk_size, chunk_overlap))\n",
        "        elif chunking_strategy == \"Recursive Python splitter with overlap\":\n",
        "            result = perform_eval(recursive_split(pages, chunk_size, chunk_overlap, Language.PYTHON))\n",
        "\n",
        "        results.append({\n",
        "            \"Chunking Strategy\": chunking_strategy,\n",
        "            \"Chunk Size\": chunk_size,\n",
        "            \"Context Precision\": result['context_precision'],\n",
        "            \"Context Recall\": result['context_recall']\n",
        "        })\n",
        "\n",
        "# Evaluate semantic chunking\n",
        "print(\"Evaluating: Semantic chunking\")\n",
        "result = perform_eval(semantic_split(pages))\n",
        "results.append({\n",
        "    \"Chunking Strategy\": \"Semantic chunking\",\n",
        "    \"Chunk Size\": \"N/A\",\n",
        "    \"Context Precision\": result['context_precision'],\n",
        "    \"Context Recall\": result['context_recall']\n",
        "})\n",
        "\n",
        "# Create DataFrame and display results\n",
        "df = pd.DataFrame(results)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Optionally, save to CSV\n",
        "df.to_csv(\"chunking_evaluation_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "Us3Hz-xqnDm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xznR5K2cnGph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z4Zx-Pn3Zm8A"
      }
    }
  ]
}